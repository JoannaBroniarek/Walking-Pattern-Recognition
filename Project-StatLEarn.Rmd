---
title: "Project - Statistical Learning"
author: "Francesco, Francisca, Ivana, Joanna, Michele,"
date: "July 5, 2019"
output:
  html_document: default
---

# Research Title

Walking patterns recognition under real world conditions


## Abstract

Classification is a fascinating world, full of things to discover and much more interesting if we can analyse
data generated by ourselves.

We have heard that as human beings we are unique, but...Will it be possible that these differences can be
detected even by a simple cell phone sensor, administered by an application designed to help young people
make better science school projects?

Our interest is to test if we are able to classify the way of walking of each one of the members of this group,
this would mean that each one of us has a unique and special way of walking and that alone would be an
interesting result.

Looking more towards the future, we think that this humble project can be the first motivation for an
advanced development in the realization of robots of each one of us, that are not only able to walk like us,
but to impersonate us whenever it is necessary.


Data collection \& source(s)
----------

After a long afternoon walking under the sun, we managed to collect all the data needed. As result we have 5 csv. files (uploaded together with this report), one for each group member. 

Each csv. contains the values of:

* Relative time: each row represents a millisecond.
* Linear Accelerometer Sensor
* Acceleration on the $x$ axis
* Acceleration on the $y$ axis
* Acceleration on the $z$ axis


-------------------------------------------------------------------

## Data Pre-processing

1. We removed from our data (directly from the Science Journal application) the beginning (1 minute) and the ending (1 minute) as we wanted to ignore time between setting a recording and walking activity.

2. In order to extract the features from the sequences of observations we used a technique of window overlap. In this technique, we divide a dataset into smaller sets such that each window has the same size controlled by parameters "window.size" and "overlap". We tried different parameters to compare the performance of our models. 

**Loading all the csv files as separated dataframes**

```{r dataframes}
## Libraries
suppressMessages(require(randomForest, quietly = T))
suppressMessages(require(caret, quietly = T))
suppressMessages(require(MASS, quietly = T))
suppressMessages(require(e1071, quietly = T))
suppressMessages(require(doParallel, quietly = T))
suppressMessages(require(corrplot, quietly = T))
suppressMessages(require(glmnet, quietly = T))


Ivana.df <- read.csv('Ivana_2walk-Samsung Recording 1-cropped.csv')
Francesco.df <- read.csv('Francesco_2walk-Samsung Recording 2-cropped.csv')
Michele.df <- read.csv('Michele_2walk-Samsung Recording 2-cropped.csv')
Francisca.df <- read.csv('Francisca_2walk-Samsung Recording 1-cropped.csv')
Joanna.df <- read.csv('Asia_2walk-Samsung Recording 1-cropped.csv')
```

**Data Visualization**

The function **acc.type.plots** is in charge of plotting each measured quantity versus the time in seconds.

```{r, fig.align="center", fig.height=6, fig.width=8}
acc.type.plots <- function(column="AccX", ylim.min = 0, ylim.max = 28){
  par(mfrow=c(3,2))
  
  plot(Ivana.df$relative_time/1000, Ivana.df[[column]], type = "l", col="red", 
       main = paste0(column, " - Ivana"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francesco.df$relative_time/1000, Francesco.df[[column]], type = "l", col="blue", 
       main = paste0(column, " - Francesco"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Michele.df$relative_time/1000, Michele.df[[column]], type = "l", col="green", 
       main = paste0(column, " - Michele"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Francisca.df$relative_time/1000, Francisca.df[[column]], type = "l", col="orchid", 
       main = paste0(column, " - Francisca"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
  plot(Joanna.df$relative_time/1000, Joanna.df[[column]], type = "l", col="orange", 
       main = paste0(column, " - Joanna"), ylim = c(ylim.min, ylim.max),
       ylab = "m/s2", xlab = "seconds")
}

acc.type.plots(column="AccX", ylim.min = -3, ylim.max = 30)
```

```{r, fig.align="center", fig.height=6, fig.width=8}
acc.type.plots(column="AccY", ylim.min = -11, ylim.max = 15)
```

```{r, fig.align="center", fig.height=6, fig.width=8}
acc.type.plots(column="AccZ", ylim.min = -30, ylim.max = 23)
```

```{r, fig.align="center", fig.height=6, fig.width=8}
acc.type.plots(column="LinearAccelerometerSensor", ylim.min = -3, ylim.max = 25)
```


-----------------------------

**Slicer** is a function for creating bins for each dataset for given parameters. 
We need to specify window.size and overlap size for dividing our data.
Function returns a list of all bins i.e. bins = [(1, 2, 3), (3, 4), (5, 6, 7, 8, 9), (10, 11, 12, 13) ...] 
where each number corresponds to a row index of dataframe.

```{r}
### It is a function that obtains possible bins for each dataset

slicer <- function(df, window.size=200, overlap = 100){
  # non-overlap size
  non_overlap <- window.size - overlap
  
  # starting index 
  from <- 1
  
  # starting relative time
  start <- df[["relative_time"]][1]
  
  # vector for created bins
  bins <- list()
  counter <- 1
  
  done <- FALSE
  while (done != TRUE) {
    # get possible bin indexes  = [start, end)
    bin.idx <- c(from, from + window.size)
    
    # get relative time indexes appropriate for that bin and save to bins vector
    bin <- c()
    for(i in start:nrow(df)) ifelse(df$relative_time[i] < bin.idx[2], bin <- c(bin, i), break)
    
    if(is.null(bin) == FALSE) {
      bins[[counter]] <- bin
      counter = counter + 1
    }
    
    # switch starting index for the next bin
    from <- from + non_overlap
    #start <- i
    start <- which(df$relative_time >= from)[1]

    # terminating condition:
    if(from > df[["relative_time"]][nrow(df)]) done <- TRUE
  }
  return(bins)
  }
```

Now, we are checking the possible number of bins generated under specific parameters.

```{r, slicer, cache=TRUE}
## Numbers of possible bins with specific window.size and overlap
window.size = 500
overlap = 200

Ivana.bins <- slicer(Ivana.df, window.size=window.size, overlap = overlap)
paste("Ivana", length(Ivana.bins))
Michele.bins <- slicer(Michele.df, window.size=window.size, overlap = overlap)
paste("Michele", length(Michele.bins))
Francesco.bins <- slicer(Francesco.df, window.size=window.size, overlap = overlap)
paste("Francesco",length(Francesco.bins))
Francisca.bins <- slicer(Francisca.df, window.size=window.size, overlap = overlap)
paste("Francisca", length(Francisca.bins))
Joanna.bins <- slicer(Joanna.df, window.size=window.size, overlap = overlap)
paste("Joanna", length(Joanna.bins))
```

This is an example of a first bin for Ivana's dataset with window.size=500 and overlap=200

```{r}
Ivana.df[Ivana.bins[[1]], ]
```

------------------------------

## Generating features

As the next step, we generated the following features:

* Average acceleration for each axis and linear acceleration

* Standard deviation for each axis and linear acceleration

* Average Absolute Difference: ${\displaystyle \frac{1}{n}\sum_{i=1}^{n}|a_{xi} - \mu|}$ where $\mu$ is the average of the axis (or linear acceleration) on the time window.



```{r}
# function which computes the absolute difference
absolute_diff <- function(xx){
  ## calculated only for rows without NANs
  xx <- na.omit(xx)
  return((1/length(xx)) * sum(abs(xx - mean(xx))))}
```


The **build.features** function takes as an input dataframe and its bins and generates a dataframe with features.


```{r}
build.features <- function(df, bins, person_ID){
  # initialize a feature dataframe
  feature.df <- data.frame()
  
  # iterate through bins, generate features and add to feature dataframe
  for(bin.ids in bins){
    bin <- df[bin.ids, ]
    created_obs <- data.frame(mean(bin$AccX, na.rm = TRUE), 
                              sd(bin$AccX, na.rm = TRUE), 
                              absolute_diff(bin$AccX), 
                              mean(bin$AccY, na.rm = TRUE),
                              sd(bin$AccY, na.rm = TRUE),
                              absolute_diff(bin$AccY), 
                              mean(bin$AccZ, na.rm = TRUE), 
                              sd(bin$AccZ, na.rm = TRUE),
                              absolute_diff(bin$AccZ), 
                              mean(bin$LinearAccelerometerSensor, na.rm = TRUE), 
                              person_ID)
    feature.df <- rbind(feature.df, created_obs)
  }
  
  names(feature.df) <- c('meanAccX', 'sdAccX', 'absdiff_AccX', 
                         'meanAccY', 'sdAccY', 'absdiff_AccY',
                         'meanAccZ', 'sdAccZ', 'absdiff_AccZ', 
                         'meanAccLin', 'person_ID')
  return(feature.df)
}


Francesco.feature.df <- build.features(Francesco.df, Francesco.bins, person_ID = 1)
Francisca.feature.df <- build.features(Francisca.df, Francisca.bins, person_ID = 2)
Ivana.feature.df <- build.features(Ivana.df, Ivana.bins, person_ID = 3)
Joanna.feature.df <- build.features(Joanna.df, Joanna.bins, person_ID = 4)
Michele.feature.df <- build.features(Michele.df, Michele.bins, person_ID = 5)
```


The **plot.features** function is in charge of plotting main four features: avg(AccX), avg(AccY), avg(AccZ) and avg(LinAcc.)

```{r, fig.align="center", fig.height=6, fig.width=8}
# function that plots main features (AccX, AccY, AccZ, LinAcc)
plot.features <- function(feature.df, person){
  plot(feature.df$meanAccX, col = "blue", ylim = c(-13,18), type = "l",
       ylab = "",  xlab = "bin")
  points(x=1:nrow(feature.df), y = feature.df$meanAccY, 
         type = "l", col = "red")
  points(x=1:nrow(feature.df), y = feature.df$meanAccZ,
         type = "l", col = "green")
  points(x=1:nrow(feature.df), y = feature.df$meanAccLin,
         type = "l", col = "orange")
  title(paste0("Main features of ", person, "`s walking pattern"))
  legend("bottomleft", legend=c("avg(AccX)", 'avg(AccY)', 'avg(AccZ)', 'avg(AccLin)'),
         col = c("blue", 'red', "green", 'orange'), lwd = c(1,1,1,1), bty = "n")
}


plot.features(Francesco.feature.df, "Francesco")
```

```{r, fig.align="center", fig.height=6, fig.width=8}
plot.features(Francisca.feature.df, "Francisca")
```

```{r, fig.align="center", fig.height=6, fig.width=8}
plot.features(Ivana.feature.df, "Ivana")
```

```{r, fig.align="center", fig.height=6, fig.width=8}
plot.features(Joanna.feature.df, "Joanna")
```

```{r, fig.align="center", fig.height=6, fig.width=8}
plot.features(Michele.feature.df, "Michele")
```



-------------------------------------

**Merging all dataframes **

All individual datasets are merged into one dataframe and re-indexed. Then, all the rows containing at least one NULL value are removed.

```{r}
walks.df <- rbind(Francesco.feature.df, 
                  Francisca.feature.df,
                  Ivana.feature.df, 
                  Joanna.feature.df, 
                  Michele.feature.df)
# removing NANs
walks.df <- na.omit(walks.df)
# reindex
rownames(walks.df) <- NULL
# show first rows 
head(walks.df)
```

```{r fig2, fig.height = 7, fig.width = 7, fig.align = "center"}
library("plot3D")
x <- walks.df$meanAccX
y <- walks.df$meanAccY
z <- walks.df$meanAccZ

scatter3D(x, y, z, bty = "b2", colvar = walks.df$person_ID, add = FALSE, pch=19, cex = 0.5,  theta = 40, phi = 20, 
          main = "Mean(AccX) vs. Mean(AccY) vs. Mean(AccZ)", 
          xlab = "mean(AccX)", xlim=c(-10, 10),
          ylab ="mean(AccY)", ylim = c(0, 10),
          zlab = "mean(AccZ)", clab = "PersonID")

```

--------------------------------

**Fractions of rows per label (person)**

```{r}
prop.table(table(walks.df$person_ID))
```

---------------------------------------------------

## **Splitting data**

Splitting dataframe in training and test sets using the **createDataPartition** function. We put 80% of the data to training set.

```{r}
set.seed(13401)

# idx for the training set
idx.train <- caret::createDataPartition(walks.df$person_ID, p = 0.8, list = FALSE)

# creating X_train and y_train sets
X_train <- walks.df[idx.train, ]
y_train <- as.factor(X_train$person_ID)
X_train$person_ID <- NULL

# creating test set
X_test <- walks.df[-idx.train, ]
y_test <- as.factor(X_test$person_ID)
X_test$person_ID <- NULL
```

-----------------------------

## **Modeling**

We are dealing with a multi-class classification. 
Used algorithms:

* Random Forest

* Linear Discriminant Analysis

* Naive Bayes (parametric)

* LASSO Logistic Regression 

* k-Nearest Neighbours

* SVM

* Stacking Model from the best 4 models


### Random Forest

Choosing the appropriate number of trees (_ntrees_) based on _Out Of Bag_ (prediction) error rates. 

```{r rf_ntrees, cache=TRUE}
# choosing the number of trees - based on X_train
RF.model <- randomForest(x = X_train, y = y_train, ntree=300, proximity=T)

head(RF.model$err.rate)
```

```{r}
# using err.rate values from RF.model we create a dataframe in order to plot 
# the OOB errors using ggplot
oob.error <- data.frame(
  nTrees = rep(1:nrow(RF.model$err.rate), times = 6),
  Error.Type = rep(c('OOB', "1", "2", "3", "4", "5"), each =nrow(RF.model$err.rate)),
  Error = c(RF.model$err.rate[, "OOB"], 
            RF.model$err.rate[, 1],
            RF.model$err.rate[, 2],
            RF.model$err.rate[, 3],
            RF.model$err.rate[, 4],
            RF.model$err.rate[, 5])
)
```

```{r}
require(ggplot2)

ggplot(data = oob.error, aes(x=nTrees, y = Error)) + 
  geom_line(aes(color = Error.Type)) + ggtitle("Error Rate vs. nTrees") + theme(plot.title = element_text(hjust = 0.5))

```

We observe that, using window.size = 500ms, the overall and relative OOB error rates stabilize for a number of trees around 80.
However, since we want to make a comparison of results for different window sizes, we need to choose a number of trees which guarantees a stable error rate for all of the cases "(i.e. we saw that for smaller window sizes the more appropriate number of trees is around 150)". 
We decided to set the parameter _ntree = 150_. 

```{r, randomForest}
randomForest.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats = TRUE){ 
  cat("\n\n\t\t\t RANDOM FOREST algorithm \n\n")
  # Model on training data
  model_RF = randomForest(x = X_train, y = y_train, ntree=150, proximity=T)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(model_RF$predicted)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Confusion matrix of the prediction (based on OOB data)
  if(full.stats==TRUE) {
  cat("\nConfusion matrix of the prediction (based on OOB data) : \n")
  print(model_RF$confusion)
  }
  ## Error rate based on OOB
  # model_RF$err.rate
  
  # Error on the training set
  err.train <- mean(as.vector(model_RF$predicted) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(err.train,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Predict from test set
  predictions <- predict(model_RF, newdata = X_test)
  
  # Error on the test set
  err.test <- mean(predictions != y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(err.test,2)))
  
  statistics <- confusionMatrix(predictions, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(predictions)
}  

RF.pred <- randomForest.fit.predict(X_train, y_train, X_test, y_test)
```

### LDA - Linear Discriminant Analysis

```{r, lda}

lda.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t LDA - LINEAR DISCRIMINANT ANALYSIS algorithm \n\n")
  # Model
  mod_LDA = lda(y_train ~ ., data = X_train)
  
  # Prediction on train set
  tr_pred_LDA = predict(mod_LDA, X_train)
  
  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred_LDA$class)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  ETr_LDA = mean(as.vector(tr_pred_LDA$class) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(ETr_LDA,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_LDA = predict(mod_LDA, X_test)
  
  # Error on the test set
  ETe_LDA <- mean(pred_LDA$class != y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(ETe_LDA,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_LDA$class, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(pred_LDA)
}

lda.pred <- lda.fit.predict(X_train, y_train, X_test, y_test)
```

### Naive Bayes (parametric)

```{r, naiveBayes}

naivebayes.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t NAIVE BAYES algorithm \n\n")
  
  # Model
  mod_NB = naiveBayes(y_train ~ ., data = X_train, type = "raw")
  
  # Prediction on train set
  tr_pred = predict(mod_NB, X_train)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(tr_pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(tr_pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_NB = predict(mod_NB, X_test)   
  
  # Error on the test set
  MCE_NB = mean(pred_NB !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(MCE_NB,2)))
  
  # Statistics
  statistics <- confusionMatrix(pred_NB, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(pred_NB)
}

naivebayes.pred <- naivebayes.fit.predict(X_train, y_train, X_test, y_test)
```

### k-Nearest Neighbor

In order to get the best tuning parameter $k$ we performed a cross validation.

```{r knnCV}
set.seed(123)
registerDoParallel(cores=1)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 2)

knn_fit <- train(y = y_train ,
                 x = X_train, method = "knn",
                 trControl  = trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
# Take a look
print(paste("Best parameter:", as.character(knn_fit$bestTune)))
plot(knn_fit, main="Accuracy vs. k", xlab="k", ylab="Accuracy")
```

Using Cross Validation method as the best parameter we received k=7.

```{r}
# function used inside knn.fit.predict
# it takes the label/class according to the highest probability
get_label <- function(predictions){
  n = nrow(predictions)
  pred_labels = rep(NA,n)
  for(i in 1:n){
    max.id <- which.max(predictions[i, ])
    pred_labels[i] <- names(predictions[i, max.id])
  }
  return(as.factor(pred_labels))
}
```


```{r knnPred}
knn.fit.predict <- function(X_train, y_train, X_test, y_test, k=knn_fit$bestTune, full.stats=TRUE){
  cat("\n\n\t\t\t k-NEAREST NEIGHBOR algorithm \n\n")
  
  # Model
  mod_KNN = knn3(y=y_train, x = X_train, k)
  
  # Prediction on train set
  tr_pred = predict(mod_KNN, X_train)
  main_tr_pred = get_label(tr_pred)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(main_tr_pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(main_tr_pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_KNN = predict(mod_KNN, X_test)
  main_pred_KNN = get_label(pred_KNN)

  # Error on the test set
  te.error = mean(main_pred_KNN !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(main_pred_KNN, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
    
  return(main_pred_KNN)
}

knn.pred <- knn.fit.predict(X_train, y_train, X_test, y_test, k = 7)
```


### LASSO Logistic Regression

```{r, lasso}

lasso.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t LASSO algorithm \n\n")
  
  mod_LASSO = cv.glmnet(as.matrix(X_train), y_train, 
                        family = "multinomial", 
                        type.measure = "class",
                        parallel = T)
  if(full.stats==TRUE) print(c("min" = mod_LASSO$lambda.min, "1se" = mod_LASSO$lambda.1se))
  
  pred_tr_LASSO = predict(mod_LASSO, newx = as.matrix(X_train), 
                          type = "class", 
                          s = mod_LASSO$lambda.1se)

  # Error on the training set
  tr.error = mean(as.vector(pred_tr_LASSO) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_LASSO = predict(mod_LASSO, newx = as.matrix(X_test), type = "class", s = mod_LASSO$lambda.1se)
  test_pred_LASSO = as.factor(pred_LASSO)

  # Error on the test set
  te.error = mean(test_pred_LASSO !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(test_pred_LASSO, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
  
  return(test_pred_LASSO)
}

lasso.pred <-lasso.fit.predict(X_train, y_train, X_test, y_test)
```

### SVM

```{r svm}

svm.fit.predict <- function(X_train, y_train, X_test, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t SVM algorithm \n\n")
  
  # Model
  mod.svm <-svm(x = X_train, y = y_train)
  
  # Prediction on train set
  train.pred <- predict(mod.svm, X_train)

  # Check if all values are predicted (no NA's)
  ifelse(sum(is.na(train.pred)) != 0, 
         ok <- "There are NAN values in predictions", 
         ok <- "All predictions evaluated")
  if(full.stats==TRUE) print(ok)
  
  # Error on the training set
  tr.error = mean(as.vector(train.pred) != as.vector(y_train))*100
  cat(sprintf("\nTrain Missclassification rate: %f", round(tr.error,2)))
  
  if(full.stats==TRUE) cat("\n\n       Predictions on the TEST set\n ------------------------------------ \n")
  
  # Prediction on test set
  pred_svm = predict(mod.svm, X_test)

  # Error on the test set
  te.error = mean(pred_svm !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(pred_svm, y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
    
  return(pred_svm)
}

svm.pred <- svm.fit.predict(X_train, y_train, X_test, y_test)
```

#### Stacking Model

We built **the stacking model** using the best four models: k-NN, Random Forest, LASSO and SVM. 
The idea is to use the predictions of majority. In a case that each classifier returns different class, we follow using the prediction of the SVM classifier. 

```{r stacking}
stacking.model <- function(knn.pred, RF.pred, lasso.pred, svm.pred){
  ## Majority voting using all the models

  stacking.pred.all <- cbind(SVM   = as.character(svm.pred),
                           knn   = as.character(knn.pred),
                           RF    = as.character(RF.pred),
                           #LDA   = as.character(lda.pred$class),
                           #NaivB = as.character(naivebayes.pred),
                           Lasso = as.character(lasso.pred)
                           )

  # finding for each row (obs) the mode
  stacking.pred.all <- as.factor( apply(stacking.pred.all, 1, function(x) names(which.max(table(x))) ) )
                  
  return(stacking.pred.all)
}

stacking.pred <- stacking.model(knn.pred, RF.pred, lasso.pred, svm.pred)
```


```{r stacking2}
stacking.statistics <- function(stacking.pred, y_test, full.stats=TRUE){
  cat("\n\n\t\t\t STACKING MODEL \n\n")
  
  te.error = mean(stacking.pred !=  y_test)*100
  cat(sprintf("\nTest Missclassification rate: %f \n\n", round(te.error,2)))

  # Statistics
  statistics <- confusionMatrix(as.factor(stacking.pred), y_test)
  if(full.stats==TRUE) print(statistics)
  else print(statistics$overall["Accuracy"])
}

stacking.statistics(stacking.pred, y_test)
```

-------------------------------------

### Summary of algorithms results:

|          | RndForest | LDA    | NaiveBayes | LASSO  | k-NN   | SVM   | Stacking |
|----------|-----------|--------|------------|--------|--------|-------|----------|
| Accuracy |   0.943   |  0.925 |    0.892   |  0.942 | 0.946  | 0.962 |   0.953  |

-----------------------------------------------

## Analysis for different bin sizes

```{r pipeline}
whole.pipeline <- function(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size, overlap, filename){
  cat(paste("#################\n","window.size  = ", window.size, "\noverlap = ", overlap, "\n#################"))
  
  ## Dividing into bins
  Ivana.bins <- slicer(Ivana.df, window.size=window.size, overlap = overlap)
  cat(paste("\nIvana", length(Ivana.bins)))
  Michele.bins <- slicer(Michele.df, window.size=window.size, overlap = overlap)
  cat(paste("\nMichele", length(Michele.bins)))
  Francesco.bins <- slicer(Francesco.df, window.size=window.size, overlap = overlap)
  cat(paste("\nFrancesco",length(Francesco.bins)))
  Francisca.bins <- slicer(Francisca.df, window.size=window.size, overlap = overlap)
  cat(paste("\nFrancisca", length(Francisca.bins)))
  Joanna.bins <- slicer(Joanna.df, window.size=window.size, overlap = overlap)
  cat(paste("\nJoanna", length(Joanna.bins)))
  
  ## Building feature dataframes
  Francesco.feature.df <- build.features(Francesco.df, Francesco.bins, person_ID = 1)
  Francisca.feature.df <- build.features(Francisca.df, Francisca.bins, person_ID = 2)
  Ivana.feature.df <- build.features(Ivana.df, Ivana.bins, person_ID = 3)
  Joanna.feature.df <- build.features(Joanna.df, Joanna.bins, person_ID = 4)
  Michele.feature.df <- build.features(Michele.df, Michele.bins, person_ID = 5)
  
  ## Merging into one dataframe
  walks.df <- rbind(Francesco.feature.df, 
                  Francisca.feature.df,
                  Ivana.feature.df, 
                  Joanna.feature.df, 
                  Michele.feature.df)
  walks.df <- na.omit(walks.df)
  rownames(walks.df) <- NULL # reindex
  
  ########     MACHINE LEARNING
  
  ## Splitting into Test Set and Train Set
  idx.train <- caret::createDataPartition(walks.df$person_ID, p = 0.8, list = FALSE)
  X_train <- walks.df[idx.train, ]
  y_train <- as.factor(X_train$person_ID)
  X_train$person_ID <- NULL

  X_test <- walks.df[-idx.train, ]
  y_test <- as.factor(X_test$person_ID)
  X_test$person_ID <- NULL
  
  ## Models and Predictions
  RF.pred <- randomForest.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  lda.pred <- lda.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  naivebayes.pred <- naivebayes.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  lasso.pred <-lasso.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)
  knn.pred <- knn.fit.predict(X_train, y_train, X_test, y_test, k=7, full.stats = FALSE) #TODO: k via CrossVal
  svm.pred <- svm.fit.predict(X_train, y_train, X_test, y_test, full.stats = FALSE)

  # Stacking model
  stacking.pred <- stacking.model(knn.pred, RF.pred, lasso.pred, svm.pred)
  stacking.statistics(stacking.pred, y_test, full.stats = FALSE)
  
  
  all.predictions <- as.data.frame(y_test)
  all.predictions <- cbind(all.predictions, 
                           as.data.frame(RF.pred), 
                           as.data.frame(lda.pred$class),
                           as.data.frame(naivebayes.pred),
                           as.data.frame(lasso.pred),
                           as.data.frame(knn.pred),
                           as.data.frame(svm.pred),
                           as.data.frame(stacking.pred)
                           )

  write.csv(x=all.predictions, file = filename, quote = FALSE)
}
```

We execute the pipeline for different parameters of window.size and overlap and save the results into csv files.

```{r, eval=FALSE}
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=200, overlap=100, filename="W200-O100_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=300, overlap=150, filename="W300-O150_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=400, overlap=200, filename="W400-O200_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=500, overlap=200, filename="W500-O200_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=600, overlap=200, filename="W600-O200_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=800, overlap=400, filename="W800-O400_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=1000, overlap=400, filename="W1000-O400_predictions.csv")
whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=1500, overlap=800, filename="W1500-O800_predictions.csv")
#whole.pipeline(Ivana.df, Michele.df, Francesco.df, Francisca.df, Joanna.df, window.size=10000, overlap=3000, filename="W10000-O3000_predictions.csv")
```

## Summaries

We plot the behaviour of accuracy of our predictions in terms of different window.size values. 
Each color corresponds to a specific classification method.

```{r}
w200 <- read.csv("W200-O100_predictions.csv")
w300 <- read.csv("W300-O150_predictions.csv")
w400 <- read.csv("W400-O200_predictions.csv")
w500 <- read.csv("W500-O200_predictions.csv")
w600 <- read.csv("W600-O200_predictions.csv")
w800 <- read.csv("W800-O400_predictions.csv")
w1000 <- read.csv("W1000-O400_predictions.csv")
w1500 <- read.csv("W1500-O800_predictions.csv")
#w10000 <- read.csv("W10000-O3000_predictions.csv")


x.bin <- c(200, 300, 400, 500, 600, 800, 1000, 1500)
y.RF <- c()
y.LDA <- c()
y.Naive.Bayes <- c()
y.LASSO <- c()
y.kNN <- c()
y.SVM <- c()
y.Stacking <- c()

for (win in list(w200, w300, w400, w500, w600, w800, w1000, w1500)){
  Random.Forest <- confusionMatrix(as.factor(win$RF.pred), as.factor(win$y_test))
  LDA <- confusionMatrix(as.factor(win$lda.pred.class), as.factor(win$y_test))
  Naive.Bayes <- confusionMatrix(as.factor(win$naivebayes.pred), as.factor(win$y_test))
  LASSO <- confusionMatrix(as.factor(win$lasso.pred), as.factor(win$y_test))
  kNN <- confusionMatrix(as.factor(win$knn.pred), as.factor(win$y_test))
  SVM <- confusionMatrix(as.factor(win$svm.pred), as.factor(win$y_test))
  Stacking <- confusionMatrix(as.factor(win$stacking.pred), as.factor(win$y_test))

  y.RF <- c(y.RF, Random.Forest$overall["Accuracy"])
  y.LDA <- c(y.LDA, LDA$overall["Accuracy"])
  y.Naive.Bayes <- c(y.Naive.Bayes, Naive.Bayes$overall["Accuracy"])
  y.LASSO <- c(y.LASSO, LASSO$overall["Accuracy"])
  y.kNN <- c(y.kNN, kNN$overall["Accuracy"])
  y.SVM <- c(y.SVM, SVM$overall["Accuracy"])
  y.Stacking <- c(y.Stacking, Stacking$overall["Accuracy"])

}


### Accuracy vs bin.size
plot(x.bin, as.vector(y.RF), type = "l", xlim = c(200, 1500), ylim=c(0.6, 1), 
     main = "Accuracy vs. Window Size", xlab = "Window size", ylab = "Accuarcy", col="red")
lines(x.bin, y.LDA, type = "l", col="orange")
lines(x.bin, y.Naive.Bayes, col = "darkblue")
lines(x.bin, y.LASSO, col="blue")
lines(x.bin, y.kNN, col = "black")
lines(x.bin, y.SVM, col= "orchid")
lines(x.bin, y.Stacking, col= "green")
legend(x="bottomright", y=0.92, legend = c( "LDA", "LASSO", "Naive.Bayes", "Random.Forest", "k-Nearest.Neighbor", "SVM", "Stacking.Model"),
       col = c("orange", 'blue', "darkblue", 'red', 'black', 'orchid', 'green'),lwd = rep(1, 6), seg.len = 1, bty = "n")

```

From the plot we can see that with the increase of window.size the accuracies of all classifiers tend to improve.
We suppose that it is due to better approximation of features, which is linked to a greater number of observations in each bin. 
For all calculations we did the best results are for window.size = 1000. 

```{r}
barplot(c(y.RF[7], y.LDA[7], y.Naive.Bayes[7], y.LASSO[7], y.kNN[7], y.SVM[7], y.Stacking[7]), 
        main = "Barplot of Accuracy values for each method \n(window.size = 1000, overlap=400)", ylab = "Accuracy",
        col = "dodgerblue", ylim = c(0.7, 1), axisnames = TRUE, xpd = F,
        names.arg = c( "LDA", "LASSO", "NB", "RF", "k-NN", "SVM", "Stack"))
```


